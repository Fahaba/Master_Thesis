% !TeX spellcheck = de_DE

\section{Ausblick} \label{chap:farsight}

Wie in der Einleitung erwähnt, gilt diese Arbeit als Versuch Basistechnologien zu präsentieren, mit denen eine CUDA-basierte positionsbezogene Schnittstelle für Audio entwickelt werden kann. Ein notwendiger Implementierungsschritt für dieses Ziel wäre die Verbindung der beiden Projekte AudioTracer und AudioParcours, sodass für die Soundmanipulation die anteiligen Frequenzen zu jedem Zeitpunkt bekannt sind. Mit diesem Schnittpunkt wäre es möglich, verschiedene Audiophänomene wie z.B. den Doppler-Effekt zu simulieren. Dafür müssten die anteiligen Frequenzen teilweise angepasst und mithilfe der inversen Fouriertransformation wieder in ein zusammengefügtes Tonbild konvertiert werden. Eine Möglichkeit für die Realisierung der hier präsentierten Technologien wäre es, spezielle Soundtreiber zu entwickelt, die direkt mit der CUDA-Schnittstelle der Grafikkarte kommunizieren können, was diesen Themenbereich jedoch nicht weniger komplex gestalten würde. Die Einleitung referenziert eine Hand voll Forschungsgebiete, in denen teure Hardware und Lokalitäten notwendig sind, um Audiophänomene zu untersuchen. Eventuell würde eine solche Schnittstelle eine \enquote{Digitalisierung} in diesen Bereichen auslösen, oder zumindest im Zusammenspiel verwendet werden, um erforschte Soundphänomene digital abbilden zu können. Das AudioParkour Projekt weist noch Erweiterungsbedarf auf. Gegenwärtig wird das WAV-Format mit einer Unterstützung von bis zu zwei Audiokanälen erwartet. Die Formastrestriktion könnte aufgehoben werden, um eine größere Breite von Audioformaten zu unterstützen. Das MP3-Format wäre ein Beispiel hierfür. Im AudioTracer Projekt wäre eine Überarbeitung der TCP-Streaming Logik sinnvoll. Bei den Testdurchläufen ist aufgefallen, dass die Daten vom StreamWriter zu langsam empfangen werden. Dadurch entsteht eine Verzögerung der CUDA-Routine. Unter anderem werden zusätzlich einige unnötige Konvertierungen zwischen Integer- und komplexen Darstellungen der Eingabefelder durchgeführt. Es wäre sinnvoll, diese initial in der komplexen Darstellungen vom StreamWriter Projekt auszulesen und auch in dieser Form zu versenden. Die CUDA-Routine des C-AudioTracer Projekts implementiert eine diskrete Fouriertransformation, bei der die jeweiligen Kernel-Threads einen nur kleinen Teilbereich der eigentlichen Samplingdaten zugewiesen bekommen. Eine individuelle Implementierung der Fast Fourier Transformation würde bei größeren Datensätzen Sinn ergeben. 

Eine weitere Möglichkeit die Performance des vorliegenden Programms zu verbessern wäre die Benutzung von \textbf{CUDA-Streams}. Das Host-System würde in diesem Fall die auszuführenden Aufgaben in einer Warteschlangenstruktur am Device-System anmelden und direkt mit der der Ausführung fortfahren, ohne auf die synchrone Ausgabe der CUDA-Routine zu warten. Dies würde den Zeitaufwand zwischen zwei Ausführungen der CUDA-Aufgaben im besten Fall komplett aufheben, sodass mehrere CUDA-Routinen ohne Aufschub ineinander überlaufen würden.

Die Allgemeine Komplexität der Initialisierung einer CUDA Kontexts in C wurde nicht gemessen, führt aber sehr wahrscheinlich dazu, dass mehr CPU Auslastung gebraucht wird als nötig wäre. Gegenwärtig wird nach einem Empfangen der Audio Samples jeweils die Initialisierung des CUDA-Kontexts neu vorgenommen. Allgemein wäre interessant zu untersuchen, die parallele Berechnung der Audiofrequenzen schneller ist als eine sequentielle Fouriertransformation. Außerdem ist aus den Ergebnissen ersichtlich, dass die errechneten Frequenzen teilweise etwas ungenau sind. Wie bereits herausgestellt wurde, liegt dies an der Größe der Eingabedaten. Hier würde ein interessanter Aspekt sein, herauszufinden, ab welcher Größe der Eingabe die Ergebnisse genauer werden. 